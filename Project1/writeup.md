## Project: Search and Sample Return

---

**The goals / steps of this project are the following:**  

**Training / Calibration**  

* Download the simulator and take data in "Training Mode"
* Test out the functions in the Jupyter Notebook provided
* Add functions to detect obstacles and samples of interest (golden rocks)
* Fill in the `process_image()` function with the appropriate image processing steps (perspective transform, color threshold etc.) to get from raw images to a map.  The `output_image` you create in this step should demonstrate that your mapping pipeline works.
* Use `moviepy` to process the images in your saved dataset with the `process_image()` function.  Include the video you produce as part of your submission.

**Autonomous Navigation / Mapping**

* Fill in the `perception_step()` function within the `perception.py` script with the appropriate image processing functions to create a map and update `Rover()` data (similar to what you did with `process_image()` in the notebook). 
* Fill in the `decision_step()` function within the `decision.py` script with conditional statements that take into consideration the outputs of the `perception_step()` in deciding how to issue throttle, brake and steering commands. 
* Iterate on your perception and decision function until your rover does a reasonable (need to define metric) job of navigating and mapping.  

[//]: # (Image References)

[image1]: ./misc/rover_image.jpg
[image2]: ./calibration_images/example_grid1.jpg
[image3]: ./calibration_images/example_rock1.jpg 
[image4]: ./output/perspective.png
[image5]: ./output/color_threshold.png
[image6]: ./output/rock.png
[image7]: ./output/rover_auto_1.png

## [Rubric](https://review.udacity.com/#!/rubrics/916/view) Points
### Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  

---
### Writeup / README

#### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one.  You can submit your writeup as markdown or pdf.  

You're reading it!

### Notebook Analysis
#### 1. Run the functions provided in the notebook on test images (first with the test data provided, next on data you have recorded). Add/modify functions to allow for color selection of obstacles and rock samples.

1) Function `perspect_transform()` convert the camera image (upper left) to top-down perspective (upper right). The mask of the camera field of view (lower left) is also calculated. It's used when filtering the obstacles. Below is the result from running this function on original camera image.  
![perspective][image4]

2) Function `color_thresh()` is left unchanged. It's used to detect the navigable terrain and obstacles. The default threshold (160, 160, 160) is reasonable good for selection the navigable terrain (left). The rest pixels are considered as obstacles. The mask generated by the function `perspect_transform()` is used to select pixels that are in the camera field of view. The code for calculating the obstacles is,
```python
obstacles = (np.ones_like(navigable) - navigable) * mask
```
![color threshold][image5]

3) A new function `rock_mask()` is added for selecting rock samples. The approach suggested in the lecture material is used. The input image is filtered by a lower and an upper threshold in HSV space, using the function `inRange()` from `cv2` library. After experimenting with the function, I find that (90, 100, 100) and (100, 255, 255) are good choices for the thresholds for selecting the rock samples. Below are the input (left) and output (right) image from this function. For later convenience, the output is converted to a binary mask, consisting only 0's and 1's.
![rock][image6]

#### 2. Populate the `process_image()` function with the appropriate analysis steps to map pixels identifying navigable terrain, obstacles and rock samples into a worldmap.  Run `process_image()` on your test data using the `moviepy` functions provided to create video output of your result. 

Following are the steps filled out in the function `process_image()`:

1) **Define source and destination points for perspective transform**
Source and destination points are defined as in the lecture material. And `dst_size = 5`, therefore the scale factor between rover coordinates and worldmap is 10.

2) **Apply perspective transform** 
I use the function `perspect_transform()`.

3) **Apply color threshold to identify navigable terrain/obstacles/rock samples**
I use the function `color_thresh()` to select navigable terrain, use the rest pixels against navigable terrain and camera mask to select obstacles, and use the function `rock_mask()` to select rock samples.

4) **Convert thresholded image pixel values to rover-centric coordinates**
I use the function `rover_coords()` for this.

5) **Convert rover-centric pixel values to world coords**
I use the function `pix_to_world()`, whre parameter `scale` is set to `10`. 

6) **Update worldmap (to be displayed on right side of screen)**
I add a new function `trusted_pixels()` to select the pixels within certain range. The selected pixels are then copied into the worldmap. In the end, the navigable terrain are compared with obstacles. If the navigable terrain wins, set the obstacles to zero.

```python
def trusted_pixels(xpix, ypix, maxdist=50):
    dist, angle = to_polar_coords(xpix, ypix)
    trust = dist < maxdist
    return np.where(trust)

# Select pixels within 5 meters
nav_trusted = trusted_pixels(navigable_x_rover, navigable_y_rover, maxdist=50)

# Copy selected pixels into worldmap
data.worldmap[navigable_y_world[nav_trusted], navigable_x_world[nav_trusted], 2] += 10

# Compare nav and obstacles
nav_like = np.where(data.worldmap[:,:,2] > data.worldmap[:,:,0])
data.worldmap[nav_like[0], nav_like[1], 0] = 0
data.worldmap[nav_like[0], nav_like[1], 2] = 10
```


### Autonomous Navigation and Mapping

#### 1. Fill in the `perception_step()` (at the bottom of the `perception.py` script) and `decision_step()` (in `decision.py`) functions in the autonomous mapping scripts and an explanation is provided in the writeup of how and why these functions were modified as they were.

The function `perception_step()` in automous mode is similar to the implementation in jupyter notebook. There is mainly one difference.

1)  When the *pitch angle* is large, the perspect transform is not very precise. Therefore the Worldmap is updated only when the *pitch angle* is within +/- 0.5 degree.


The function `decision_step()` in _decision.py_:

1) Add a new branch in the decision tree, to avoid rover running in circles. Rover's steering angles are monitored. Once the last 100 steering angles are all of the same value (+/- 15), the rover is put into *stop* mode. A new attribute `last_steers` is introduced in the rover object, and the function `update_rover()` in _supporting_function.py_ is improved for the purpose of monitoring. 
```python
# in class Roverstate
self.last_steers = deque([0] * 100)

# in function update_rover()
Rover.last_steers.append(Rover.steer)
Rover.last_steers.popleft()

# in decision_step()
if np.all(np.absolute(np.array(Rover.last_steers)) == 15.0) and Rover.vel > 0.5:
    Rover.throttle = 0
    Rover.brake = Rover.brake_set
    Rover.mode = 'stop'
```


#### 2. Autonomous mode 

> Simulator settings: resolution is 1280 x 768, quality is good, fps is about 17~20.

##### Filedity and Mapped Area

1) With the function `trusted_pixels()` described in the last section, the worldmap is updated using pixels within certain range. The range for navigable terrain is 3 meters, and for obstacles in 20 meters. The detection is more precise, and the fidelity is improved by about 10%. Also it helps to prevent the navigable pixels falsely labeled as obstacles, the mapped area is also improved.

2) I noticed that, the filelity can drop suddenly when the rover switches from _stop_ mode to _forward_ mode. It leads to quick changes in pitch angles in a short time. To avoid such problems, worldmap is updated only when the pitch angles are within +/- 0.5 degree. It's very effective to keep the fidelity stable. 

##### Decision Step

1) When the rover moves in a large open space, it can happen that the rover keeps moving in a circle. This is due to the steering angle is locked at +/- 15 degree. I have implemented a simple solution: monitoring the steering angle and once it is locked for 100 frames, change rover to _stop_ mode.
The screenshot below shows such a situation, the steering angle has been locked at 15 degree, and the rover has switched to _stop_ mode.
As a simple solution, it avoids endless circling. But it does not really provide smart navigation as well.
There can be better solution. E.g., direct the rover to a random direction, weighted by the `nav_angles`. 
![auto mode][image7]

2) Another issue the rover can run into, is that it tends to cover part of the map repeatly. It should be improved to let the rover cover more area easily. For example by navigate the rover to the direction which has not been searched.
 
